{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Memotion Analysis\n",
        "**Final Assignment**  \n",
        "Tasks:  \n",
        "A. Sentiment Classification  \n",
        "B. Humor/Sarcasm/Offense Detection  \n",
        "C. Motivational Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration & Imports\n",
        "Import required libraries and set up hardware acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageFile\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, ViTImageProcessor, ViTModel\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Enable truncated image handling\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# Hardware configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading DistilBert and ViT transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, labels_path, image_dir, text_max_length=128):\n",
        "        self.labels_df = pd.read_csv(labels_path)\n",
        "        self.image_dir = image_dir\n",
        "        \n",
        "        # Data validation\n",
        "        self.labels_df['text_corrected'] = self.labels_df['text_corrected'].astype(str)\n",
        "        \n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_max_length = text_max_length\n",
        "        \n",
        "        self.image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        \n",
        "        self.label_maps = {\n",
        "            'sentiment': ['very_negative', 'negative', 'neutral', 'positive', 'very_positive'],\n",
        "            'humor': ['not_funny', 'funny', 'very_funny', 'hilarious'],\n",
        "            'sarcasm': ['not_sarcastic', 'general', 'twisted_meaning', 'very_twisted'],\n",
        "            'offensive': ['not_offensive', 'slight', 'very_offensive', 'hateful_offensive'],\n",
        "            'motivational': ['not_motivational', 'motivational']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.labels_df.iloc[idx]\n",
        "        \n",
        "        # Text processing\n",
        "        text = str(row['text_corrected'])\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.text_max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Image processing\n",
        "        img_path = os.path.join(self.image_dir, row['image_name'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        pixel_values = self.image_processor(images=image, return_tensors=\"pt\").pixel_values\n",
        "        \n",
        "        # Label encoding\n",
        "        labels = {\n",
        "            'sentiment': torch.tensor(self.label_maps['sentiment'].index(row['overall_sentiment']), dtype=torch.long),\n",
        "            'humor': torch.tensor(self.label_maps['humor'].index(row['humour']), dtype=torch.long),\n",
        "            'sarcasm': torch.tensor(self.label_maps['sarcasm'].index(row['sarcasm']), dtype=torch.long),\n",
        "            'offensive': torch.tensor(self.label_maps['offensive'].index(row['offensive']), dtype=torch.long),\n",
        "            'motivational': torch.tensor(1 if row['motivational'] == 'motivational' else 0, dtype=torch.long)\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'pixel_values': pixel_values.squeeze(),\n",
        "            'labels': labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fusion Layer with Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Text encoder\n",
        "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        \n",
        "        # Image encoder\n",
        "        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        \n",
        "        # Multimodal fusion\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(768*2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        \n",
        "        # Classification heads\n",
        "        self.classifier = nn.ModuleDict({\n",
        "            'sentiment': nn.Linear(512, 5),\n",
        "            'humor': nn.Linear(512, 4),\n",
        "            'sarcasm': nn.Linear(512, 4),\n",
        "            'offensive': nn.Linear(512, 4),\n",
        "            'motivational': nn.Linear(512, 2)\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        text_out = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:,0,:]\n",
        "        image_out = self.image_model(pixel_values=pixel_values).last_hidden_state[:,0,:]\n",
        "        \n",
        "        fused = torch.cat([text_out, image_out], dim=1)\n",
        "        fused = self.fusion(fused)\n",
        "        \n",
        "        return {task: self.classifier[task](fused) for task in self.classifier}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the model to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, optimizer, criterion, epochs=2):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        \n",
        "        for batch in progress_bar:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = {k: v.to(device) for k, v in batch['labels'].items()}\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "            loss = sum(criterion[task](outputs[task], labels[task]) for task in outputs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "        \n",
        "        print(f\"Epoch {epoch+1} Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the datasets to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:  34%|███▎      | 147/437 [08:17<16:18,  3.37s/it, loss=5.6019]c:\\Users\\karti\\anaconda3\\envs\\ML-env\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 1/2: 100%|██████████| 437/437 [25:00<00:00,  3.43s/it, loss=5.6098]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Avg Loss: 5.6426\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 437/437 [24:25<00:00,  3.35s/it, loss=5.6576]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Avg Loss: 5.5518\n",
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize components\n",
        "dataset = MemeDataset(\n",
        "    labels_path='labels.csv',\n",
        "    image_dir='images'\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "model = MultimodalModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Loss functions\n",
        "criterion = {\n",
        "    task: nn.CrossEntropyLoss()\n",
        "    for task in ['sentiment', 'humor', 'sarcasm', 'offensive', 'motivational']\n",
        "}\n",
        "\n",
        "# Start training\n",
        "trained_model = train_model(model, dataloader, optimizer, criterion)\n",
        "\n",
        "# Save model\n",
        "torch.save(trained_model.state_dict(), 'memotion_model.pth')\n",
        "print(\"Model saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
